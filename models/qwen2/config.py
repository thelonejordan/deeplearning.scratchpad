from __future__ import annotations
from typing import Optional
from dataclasses import dataclass

@dataclass
class QwenConfig:
  dim: int
  hidden_dim: int
  n_layers: int
  n_heads: int
  n_kv_heads: int
  vocab_size: int
  max_position_embeddings: int
  norm_eps: float
  tie_word_embeddings: bool = False
  # these are constant across all models
  rope_theta: float = 1000000.
  torch_dtype: str = "bfloat16"
  # these are populated through __post_init__
  head_dim: Optional[int] = None
  # these are populated from runtime args
  max_seq_len: Optional[int] = None
  max_batch_size: Optional[int] = None

  def __post_init__(self):
    assert self.vocab_size > 0, self.vocab_size
    assert self.max_seq_len is not None and self.max_batch_size is not None, (self.max_seq_len, self.max_batch_size)
    assert self.max_seq_len > 0 and self.max_seq_len <= self.max_position_embeddings, (
      f"max_seq_len must be between 1 and {self.max_position_embeddings}, got {self.max_seq_len}.")
    assert self.max_batch_size > 0, self.max_batch_size

    assert self.head_dim is None
    assert self.dim % self.n_heads == 0
    self.head_dim = self.dim // self.n_heads
    if self.n_kv_heads is None: self.n_kv_heads = self.n_heads

  @staticmethod
  def build(max_seq_len: int=2048, max_batch_size: int=32, **params) -> QwenConfig:
    return QwenConfig(max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params)


# Coder, Maths and other variants are supported but not listed here

CONFIGS = {
  "Qwen/Qwen2-0.5B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 896,
    hidden_dim = 4864,
    max_position_embeddings = 131072,
    max_window_layers = 24,
    n_heads = 14,
    n_layers = 24,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2-0.5B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 896,
    hidden_dim = 4864,
    max_position_embeddings = 32768,
    max_window_layers = 24,
    n_heads = 14,
    n_layers = 24,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2-1.5B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 1536,
    hidden_dim = 8960,
    max_position_embeddings = 131072,
    max_window_layers = 28,
    n_heads = 12,
    n_layers = 28,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2-1.5B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 1536,
    hidden_dim = 8960,
    max_position_embeddings = 32768,
    max_window_layers = 28,
    n_heads = 12,
    n_layers = 28,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2-7B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 3584,
    hidden_dim = 18944,
    max_position_embeddings = 131072,
    max_window_layers = 28,
    n_heads = 28,
    n_layers = 28,
    n_kv_heads = 4,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2-7B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 3584,
    hidden_dim = 18944,
    max_position_embeddings = 32768,
    max_window_layers = 28,
    n_heads = 28,
    n_layers = 28,
    n_kv_heads = 4,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2-72B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 8192,
    hidden_dim = 29568,
    max_position_embeddings = 131072,
    max_window_layers = 80,
    n_heads = 64,
    n_layers = 80,
    n_kv_heads = 8,
    norm_eps = 1e-05,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2-72B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 8192,
    hidden_dim = 29568,
    max_position_embeddings = 32768,
    max_window_layers = 80,
    n_heads = 64,
    n_layers = 80,
    n_kv_heads = 8,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-0.5B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 896,
    hidden_dim = 4864,
    max_position_embeddings = 32768,
    max_window_layers = 24,
    n_heads = 14,
    n_layers = 24,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2.5-0.5B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 896,
    hidden_dim = 4864,
    max_position_embeddings = 32768,
    max_window_layers = 21,
    n_heads = 14,
    n_layers = 24,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2.5-1.5B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 1536,
    hidden_dim = 8960,
    max_position_embeddings = 131072,
    max_window_layers = 28,
    n_heads = 12,
    n_layers = 28,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2.5-1.5B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 1536,
    hidden_dim = 8960,
    max_position_embeddings = 32768,
    max_window_layers = 21,
    n_heads = 12,
    n_layers = 28,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2.5-3B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 2048,
    hidden_dim = 11008,
    max_position_embeddings = 32768,
    max_window_layers = 36,
    n_heads = 16,
    n_layers = 36,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2.5-3B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 2048,
    hidden_dim = 11008,
    max_position_embeddings = 32768,
    max_window_layers = 70,
    n_heads = 16,
    n_layers = 36,
    n_kv_heads = 2,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 151936,
    tie_word_embeddings = True,
  ),
  "Qwen/Qwen2.5-7B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 3584,
    hidden_dim = 18944,
    max_position_embeddings = 131072,
    max_window_layers = 28,
    n_heads = 28,
    n_layers = 28,
    n_kv_heads = 4,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-7B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 3584,
    hidden_dim = 18944,
    max_position_embeddings = 32768,
    max_window_layers = 28,
    n_heads = 28,
    n_layers = 28,
    n_kv_heads = 4,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-14B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 5120,
    hidden_dim = 13824,
    max_position_embeddings = 131072,
    max_window_layers = 48,
    n_heads = 40,
    n_layers = 48,
    n_kv_heads = 8,
    norm_eps = 1e-05,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-14B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 5120,
    hidden_dim = 13824,
    max_position_embeddings = 131072,
    max_window_layers = 48,
    n_heads = 40,
    n_layers = 48,
    n_kv_heads = 8,
    norm_eps = 1e-05,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-32B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 5120,
    hidden_dim = 27648,
    max_position_embeddings = 131072,
    max_window_layers = 64,
    n_heads = 40,
    n_layers = 64,
    n_kv_heads = 8,
    norm_eps = 1e-05,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-32B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 5120,
    hidden_dim = 27648,
    max_position_embeddings = 32768,
    max_window_layers = 70,
    n_heads = 40,
    n_layers = 64,
    n_kv_heads = 8,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-72B": dict(
    bos_token_id = 151643,
    eos_token_id = 151643,
    dim = 8192,
    hidden_dim = 29568,
    max_position_embeddings = 131072,
    max_window_layers = 80,
    n_heads = 64,
    n_layers = 80,
    n_kv_heads = 8,
    norm_eps = 1e-05,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-72B-Instruct": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 8192,
    hidden_dim = 29568,
    max_position_embeddings = 32768,
    max_window_layers = 70,
    n_heads = 64,
    n_layers = 80,
    n_kv_heads = 8,
    norm_eps = 1e-06,
    rope_theta = 1000000.0,
    sliding_window = 131072,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-7B-Instruct-1M": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 3584,
    hidden_dim = 18944,
    max_position_embeddings = 1010000,
    max_window_layers = 28,
    n_heads = 28,
    n_layers = 28,
    n_kv_heads = 4,
    norm_eps = 1e-05,
    rope_theta = 10000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/Qwen2.5-14B-Instruct-1M": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 5120,
    hidden_dim = 13824,
    max_position_embeddings = 1010000,
    max_window_layers = 48,
    n_heads = 40,
    n_layers = 48,
    n_kv_heads = 8,
    norm_eps = 1e-05,
    rope_theta = 10000000.0,
    sliding_window = 1010000,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/QwQ-32B-Preview": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 5120,
    hidden_dim = 27648,
    max_position_embeddings = 32768,
    max_window_layers = 64,
    n_heads = 40,
    n_layers = 64,
    n_kv_heads = 8,
    norm_eps = 1e-05,
    rope_theta = 1000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
  "Qwen/QwQ-32B": dict(
    bos_token_id = 151643,
    eos_token_id = 151645,
    dim = 5120,
    hidden_dim = 27648,
    max_position_embeddings = 40960,
    max_window_layers = 64,
    n_heads = 40,
    n_layers = 64,
    n_kv_heads = 8,
    norm_eps = 1e-05,
    rope_theta = 1000000.0,
    sliding_window = 32768,
    torch_dtype = "bfloat16",
    vocab_size = 152064,
    tie_word_embeddings = False,
  ),
}
