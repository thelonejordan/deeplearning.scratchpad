{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6e83ba-143b-4e74-b454-6fd54d709f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fa210b-3099-48b4-abf3-532d52483e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c160e6e-1ce2-407d-bec1-86595e97a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llama3.transformer import Transformer\n",
    "from models.llama3.tokenizer import Tokenizer\n",
    "from models.llama3.config import LlamaConfig\n",
    "from models.llama3.load import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28d3158-5d81-45d6-8875-d0f3f8ffdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_main(prompts):\n",
    "    device = \"mps\"\n",
    "    model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    # prompts = [\"The theory of relativity states that\"]\n",
    "    print(prompts[0])\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\")\n",
    "    print(inputs[\"input_ids\"].numpy().tolist())\n",
    "    # [[128000, 791, 10334, 315, 1375, 44515, 5415, 430]]\n",
    "    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "    model = LlamaForCausalLM.from_pretrained(model_id).to(device)\n",
    "    model.generation_config.pad_token_id = model.config.eos_token_id\n",
    "    \"\"\"\n",
    "    /Users/jyotirmaya.mahanta/projects/thelonejordan/personal/deeplearning.scratchpad/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
    "    warnings.warn(\n",
    "    /Users/jyotirmaya.mahanta/projects/thelonejordan/personal/deeplearning.scratchpad/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
    "    warnings.warn(\n",
    "    \"\"\"\n",
    "    outputs = model.generate(**inputs, max_length=30, do_sample=False, temperature=None, top_p=None)\n",
    "    print(model.generation_config)\n",
    "    print(outputs.cpu().numpy().tolist())\n",
    "    # [[128000, 791, 10334, 315, 1375, 44515, 5415, 430, 279, 4732, 315, 3177, 374, 6926, 304, 682, 5905, 14418, 13, 1115, 3445, 430, 422, 499, 527, 7366, 520, 264, 6926, 4732]]\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(texts[0])\n",
    "    # The theory of relativity states that the speed of light is constant in all reference frames. This means that if you are moving at a constant speed\n",
    "    return model, tokenizer, model.config, model.generation_config, device, inputs, texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d526811-0ff2-4dce-bb24-782a6df2bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate(prompt_tokens: list[str],model: Transformer, tokenizer: Tokenizer, config: LlamaConfig, device, logprobs: bool=False):\n",
    "  max_batch_size, max_seq_len = config.max_batch_size, config.max_seq_len\n",
    "  bsz = len(prompt_tokens)\n",
    "  assert bsz <= max_batch_size, (bsz, max_batch_size)\n",
    "  max_gen_len = config.max_seq_len\n",
    "  min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "  max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "  assert max_prompt_len <= max_seq_len\n",
    "  total_len = min(max_seq_len, max_gen_len + max_prompt_len)\n",
    "  pad_id = tokenizer.pad_id\n",
    "  tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=device)\n",
    "  for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "  if logprobs:\n",
    "    token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "  prev_pos = 0\n",
    "  eos_reached = torch.tensor([False] * bsz, device=device)\n",
    "  input_text_mask = tokens != pad_id\n",
    "  if min_prompt_len == total_len:\n",
    "    logits = model.forward(tokens, prev_pos)\n",
    "    token_logprobs = -F.cross_entropy(\n",
    "      input=logits.transpose(1, 2),\n",
    "      target=tokens,\n",
    "      reduction=\"none\",\n",
    "      ignore_index=pad_id,\n",
    "    )\n",
    "  stop_tokens = torch.tensor(list(tokenizer.stop_tokens), device=device)\n",
    "\n",
    "  for cur_pos in range(min_prompt_len, total_len):\n",
    "    logits = model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "    probs = torch.softmax(logits[:, -1], dim=-1)\n",
    "    next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "    next_token = next_token.reshape(-1)\n",
    "    # only replace token if prompt has already been generated\n",
    "    next_token = torch.where(\n",
    "      input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "    )\n",
    "    tokens[:, cur_pos] = next_token\n",
    "    if logprobs:\n",
    "      token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "        input=logits.transpose(1, 2),\n",
    "        target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "        reduction=\"none\",\n",
    "        ignore_index=pad_id,\n",
    "      )\n",
    "    eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "      torch.isin(next_token, stop_tokens)\n",
    "    )\n",
    "    prev_pos = cur_pos\n",
    "    if all(eos_reached):\n",
    "      break\n",
    "  if logprobs:\n",
    "    token_logprobs = token_logprobs.tolist()  # type: ignore\n",
    "  out_tokens, out_logprobs = [], []\n",
    "  for i, toks in enumerate(tokens.tolist()):\n",
    "    # cut to max gen len\n",
    "    start = 0\n",
    "    toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "    if logprobs:\n",
    "      probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "    # cut to after eos tok if any\n",
    "    for stop_token in tokenizer.stop_tokens:\n",
    "      try:\n",
    "        eos_idx = toks.index(stop_token)\n",
    "        toks = toks[:eos_idx]\n",
    "        if logprobs:\n",
    "          probs = probs[:eos_idx]\n",
    "      except ValueError:\n",
    "        pass\n",
    "    out_tokens.append(toks)\n",
    "    out_logprobs.append(probs)\n",
    "  return out_tokens, (out_logprobs if logprobs else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18a5e0b5-f9e9-45c6-b9b6-fed505aae88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def my_main(prompts, safetensors=False):\n",
    "    device = \"mps\"\n",
    "    model, tokenizer, config = build(\n",
    "        max_seq_len=30,\n",
    "        max_batch_size=1,\n",
    "        model_desc=\"3B\",\n",
    "        version=2,\n",
    "        safetensors=safetensors,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    # prompts = [\"The theory of relativity states that\"]\n",
    "    print(prompts[0])\n",
    "    tokenizer.pad_id = tokenizer.eos_id\n",
    "    inputs = [tokenizer.encode(s, bos=True, eos=False) for s in prompts]\n",
    "    print(inputs)\n",
    "    # [[128000, 791, 10334, 315, 1375, 44515, 5415, 430]]\n",
    "    outputs, logprobs = generate(inputs, model, tokenizer, config, device, logprobs=False)\n",
    "    print(outputs)\n",
    "    # Before rope fix:\n",
    "    # [[128000, 791, 10334, 315, 1375, 44515, 5415, 430, 279, 4732, 315, 3177, 374, 6926, 304, 682, 5905, 14418, 315, 5905, 13, 578, 10334, 315, 1375, 44515, 374, 264, 10334, 315]]\n",
    "    # After rope fix:\n",
    "    # [128000, 791, 10334, 315, 1375, 44515, 5415, 430, 279, 4732, 315, 3177, 374, 6926, 304, 682, 5905, 14418, 13, 1115, 3445, 430, 422, 499, 527, 7366, 520, 264, 6926, 4732]]\n",
    "    outputs = [i[i.index(tokenizer.bos_id)+1:]for i in outputs]\n",
    "    texts = [tokenizer.decode(toks) for toks in outputs]\n",
    "    print(texts[0])\n",
    "    # print(np.exp(np.array(logprobs, dtype=np.float32)).tolist())\n",
    "    return model, tokenizer, config, None, device, inputs, texts[0]\n",
    "\n",
    "\n",
    "my_main_torch = functools.partial(my_main, safetensors=False)\n",
    "my_main_safetensors = functools.partial(my_main, safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12436f23-a671-47f1-8444-db2ab5c691b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "@dataclass\n",
    "class Benchmark:\n",
    "    model: torch.nn.Module\n",
    "    tokenizer: Any\n",
    "    config: Any\n",
    "    generation_config: Any\n",
    "    device: Any\n",
    "    inp: list[list[int]]\n",
    "    out: list[list[int]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde47642-4919-4ca0-83d9-3c568b4c929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\"The theory of relativity states that\"]\n",
    "prompts = [\"The theory of relativity states that the speed of light is constant in all reference frames\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540a60d1-00c8-45c8-b23c-b488a3f2752a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theory of relativity states that the speed of light is constant in all reference frames\n",
      "[[128000, 791, 10334, 315, 1375, 44515, 5415, 430, 279, 4732, 315, 3177, 374, 6926, 304, 682, 5905, 14418]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78def92ad3e74e1e8d32904ceeedb87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"pad_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[[128000, 791, 10334, 315, 1375, 44515, 5415, 430, 279, 4732, 315, 3177, 374, 6926, 304, 682, 5905, 14418, 13, 1115, 3445, 430, 422, 499, 527, 7366, 520, 264, 6926, 4732]]\n",
      "The theory of relativity states that the speed of light is constant in all reference frames. This means that if you are moving at a constant speed\n"
     ]
    }
   ],
   "source": [
    "bench_hf = Benchmark(*hf_main(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a2683f6-aa00-4faf-b8c8-203eaa700f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e021e9dfda48628df686430bdb3ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8e703330c348abbded9e178ebf8e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded tiktoken model from /Users/jyotirmaya.mahanta/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062/original/tokenizer.model\n",
      "#words: 128256 - BOS ID: 128000 - EOS ID: 128001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c7c5c535da4b71b452b977a0df934e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.21B\n",
      "The theory of relativity states that the speed of light is constant in all reference frames\n",
      "[[128000, 791, 10334, 315, 1375, 44515, 5415, 430, 279, 4732, 315, 3177, 374, 6926, 304, 682, 5905, 14418]]\n",
      "[[128000, 791, 10334, 315, 1375, 44515, 5415, 430, 279, 4732, 315, 3177, 374, 6926, 304, 682, 5905, 14418, 13, 1115, 3445, 430, 422, 499, 527, 7366, 520, 264, 6926, 4732]]\n",
      "The theory of relativity states that the speed of light is constant in all reference frames. This means that if you are moving at a constant speed\n"
     ]
    }
   ],
   "source": [
    "bench_cu = Benchmark(*my_main_safetensors(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c62323af-91cf-414c-a1b4-c3db31d2962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bench_hf.out == bench_cu.out, \"output mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950b1b57-71f6-4aaa-9f53-fa93f6f190dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del bench_hf\n",
    "del bench_cu\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
