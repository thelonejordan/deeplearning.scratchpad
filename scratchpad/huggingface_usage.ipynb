{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced54bfb-d0bd-48a1-8e28-468c7e1f0175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa60d37e6f07464c8f78fc2bc057294a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "pipeline.device=device(type='mps')\n",
      "pipeline.model.dtype=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/en/model_doc/llama3\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "print(pipeline.generation_config)\n",
    "print(f\"{pipeline.device=}\")\n",
    "print(f\"{pipeline.model.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067ab8d5-39ea-48cc-8754-eb3a8f0a361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128001, 128001, 128001)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.model.config.eos_token_id, pipeline.tokenizer.eos_token_id, pipeline.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4253b9df-1f34-450c-86fd-789cd9e8bc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that the speed of light is constant, and all objects move at a constant speed with respect to the speed of light. In other words, the speed of light is the same for all observers, regardless of their state of motion.\n",
      "This theory was first proposed by Albert Einstein\n",
      "\n",
      "Simply put, the theory of relativity states that the laws of physics are the same for all non-accelerating observers, and that the speed of light in a vacuum is the same for all non-accelerating observers, regardless of their relative velocities. The theory applies to all frames of reference and is\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Simply put, the theory of relativity states that\"\n",
    "\n",
    "# need to set both to supress warning\n",
    "pipeline.model.generation_config.pad_token_id = pipeline.model.config.eos_token_id\n",
    "pipeline.tokenizer.pad_token_id=pipeline.model.config.eos_token_id\n",
    "\n",
    "outputs = pipeline(input_text, batch_size=1, num_return_sequences=2, padding=True, truncation=True, max_length=64)\n",
    "for output in outputs:\n",
    "    print(output['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43012c62-201d-4fd7-b43c-c5d9ff72c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that the speed of light is the same for all observers, regardless of their state of motion. This means that the speed of light is constant, regardless of the speed of the observer. This theory has been confirmed by many experiments, including the Michelson-Morley experiment\n",
      "\n",
      "The phenomenon of global warming refers to the phenomenon of the Earth’s temperature increasing due to the greenhouse effect. The greenhouse effect is the process by which the Earth’s temperature is regulated by the atmosphere. The atmosphere traps the heat that the sun radiates and the heat that the Earth emits. The greenhouse effect is\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\n",
    "  \"Simply put, the theory of relativity states that\",\n",
    "  \"The phenomenon of global warming refers to the\",\n",
    "]\n",
    "\n",
    "pipeline.tokenizer.padding_side = \"left\"\n",
    "outputs = pipeline(input_texts, batch_size=2, padding=True, truncation=True, max_length=64)\n",
    "for output in outputs:\n",
    "    print(output[0]['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4f7b25-eb8a-4ae8-83ad-35f27266c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ae957b-c75d-4506-a568-fca6659699a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
      "<code object __call__ at 0x121a31a00, file \"/Users/jyotirmaya.mahanta/projects/thelonejordan/personal/deeplearning.scratchpad/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2783>\n",
      "\n",
      "bos_token,\t<|begin_of_text|>,\t128000\n",
      "eos_token,\t<|end_of_text|>,\t128001\n",
      "\n",
      "tokenizer.padding_side='left'\n",
      "tokenizer.eos_token='<|end_of_text|>'\n",
      "tokenizer.bos_token='<|begin_of_text|>'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "print(tokenizer.__class__)\n",
    "print(tokenizer.__call__.__code__)\n",
    "print()\n",
    "for name, token in tokenizer.special_tokens_map.items():\n",
    "    print(name, token, tokenizer.convert_tokens_to_ids(token), sep=\",\\t\")\n",
    "print()\n",
    "print(f\"{tokenizer.padding_side=}\")\n",
    "print(f\"{tokenizer.eos_token=}\")\n",
    "print(f\"{tokenizer.bos_token=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7295651-bf00-4663-8bbc-00dd818e319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 61346, 2231, 11, 279, 10334, 315, 1375, 44515, 5415, 430]\n",
      "[128001, 128001, 128000, 791, 25885, 315, 3728, 24808, 19813, 311, 279]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "<|begin_of_text|>Simply put, the theory of relativity states that\n",
      "<|end_of_text|><|end_of_text|><|begin_of_text|>The phenomenon of global warming refers to the\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "# inputs = tokenizer.tokenize(prompts, return_tensors=\"np\", truncation=True, padding=True, max_length=64)\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "# print(inputs)\n",
    "token_ids = [i.tolist() for i in inputs[\"input_ids\"]]\n",
    "attention_mask = [i.tolist() for i in inputs[\"attention_mask\"]]\n",
    "print(\"\\n\".join([str(ids) for ids in token_ids]))\n",
    "print(\"\\n\".join([str(ids) for ids in attention_mask]))\n",
    "print()\n",
    "outputs = tokenizer.batch_decode(token_ids, skip_special_tokens=False)\n",
    "print(\"\\n\".join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "babc10f0-7648-4a51-9a16-1718614a1ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ca78878647459f854526a1c6588ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e38557-94f9-4e37-8691-25bdb6ae7127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2dc81d3-9b49-4234-8905-66bef6525b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.device=device(type='mps', index=0)\n",
      "model.dtype=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model.device=}\")\n",
    "print(f\"{model.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed2adc1c-ff44-46aa-8bfb-78040e6612ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained.example\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config, unused_kwargs = GenerationConfig.from_pretrained(model_id, return_unused_kwargs=True)\n",
    "print(generation_config)\n",
    "print(unused_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98efdc97-356d-4f50-a3ff-a5cd2f16a85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simply put, the theory of relativity states that the speed of light is constant, regardless of the velocity of the observer.\n",
      "The speed of light is 299,792,458 meters per second. From an observer at rest, the speed of light is always the same. But if we try to measure the speed\n",
      "\n",
      "The phenomenon of global warming refers to the increase in the average temperature of the earth's surface over the past 50 years. According to the Intergovernmental Panel on Climate Change (IPCC), global warming is caused by the burning of fossil fuels, and especially the release of carbon dioxide (CO2\n"
     ]
    }
   ],
   "source": [
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "# print(inputs)\n",
    "\n",
    "# need this again to suppress warning\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate\n",
    "# https://github.com/huggingface/transformers/blob/a22a4378d97d06b7a1d9abad6e0086d30fdea199/src/transformers/generation/utils.py#L1914\n",
    "# `**kwargs` passed to generate matching the attributes of `generation_config` will override them.\n",
    "output_ids = model.generate(**inputs, generation_config=generation_config, max_length=64, temperature=0.9)\n",
    "print()\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "print(\"\\n\\n\".join(outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
