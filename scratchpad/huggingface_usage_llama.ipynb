{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced54bfb-d0bd-48a1-8e28-468c7e1f0175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ec726118a2410aa62fdbe582cd650b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "pipeline.device=device(type='mps')\n",
      "pipeline.model.dtype=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/en/model_doc/llama3\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "print(pipeline.generation_config)\n",
    "print(f\"{pipeline.device=}\")\n",
    "print(f\"{pipeline.model.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067ab8d5-39ea-48cc-8754-eb3a8f0a361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128001, 128001, 128001)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.model.config.eos_token_id, pipeline.tokenizer.eos_token_id, pipeline.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4253b9df-1f34-450c-86fd-789cd9e8bc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that the speed of light is constant. This is an extremely important and revolutionary concept in physics and mathematics. The theory of relativity is based on the assumption that all observers will agree on the speed of light, and this is true for all observers. The speed of light\n",
      "\n",
      "Simply put, the theory of relativity states that the speed of light is constant for all observers, regardless of the observer's speed. In other words, the speed of light is the same for all observers. This is a fundamental principle of physics and is one of the most important ideas in modern physics. The theory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Simply put, the theory of relativity states that\"\n",
    "\n",
    "# need to set both to supress warning\n",
    "pipeline.model.generation_config.pad_token_id = pipeline.model.config.eos_token_id\n",
    "pipeline.tokenizer.pad_token_id=pipeline.model.config.eos_token_id\n",
    "\n",
    "outputs = pipeline(input_text, batch_size=1, num_return_sequences=2, padding=True, truncation=True, max_length=64)\n",
    "for output in outputs:\n",
    "    print(output['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43012c62-201d-4fd7-b43c-c5d9ff72c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that the laws of physics are the same for all observers, as long as they all move at the same speed relative to each other. This is a very difficult concept to understand, but the most important thing to remember is that nothing can travel faster than the speed of light\n",
      "\n",
      "The phenomenon of global warming refers to the warming of the Earthâ€™s climate over the past century. Scientists have identified a number of causes of global warming, including the release of greenhouse gases such as carbon dioxide and methane into the atmosphere. These gases trap heat, causing the Earth to warm. Other causes of global\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\n",
    "  \"Simply put, the theory of relativity states that\",\n",
    "  \"The phenomenon of global warming refers to the\",\n",
    "]\n",
    "\n",
    "pipeline.tokenizer.padding_side = \"left\"\n",
    "outputs = pipeline(input_texts, batch_size=2, padding=True, truncation=True, max_length=64)\n",
    "for output in outputs:\n",
    "    print(output[0]['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4f7b25-eb8a-4ae8-83ad-35f27266c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ae957b-c75d-4506-a568-fca6659699a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
      "<code object __call__ at 0x123285020, file \"/Users/jyotirmaya.mahanta/projects/thelonejordan/personal/deeplearning.scratchpad/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2783>\n",
      "\n",
      "bos_token,\t<|begin_of_text|>,\t128000\n",
      "eos_token,\t<|end_of_text|>,\t128001\n",
      "\n",
      "tokenizer.padding_side='left'\n",
      "tokenizer.eos_token='<|end_of_text|>'\n",
      "tokenizer.bos_token='<|begin_of_text|>'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "print(tokenizer.__class__)\n",
    "print(tokenizer.__call__.__code__)\n",
    "print()\n",
    "for name, token in tokenizer.special_tokens_map.items():\n",
    "    print(name, token, tokenizer.convert_tokens_to_ids(token), sep=\",\\t\")\n",
    "print()\n",
    "print(f\"{tokenizer.padding_side=}\")\n",
    "print(f\"{tokenizer.eos_token=}\")\n",
    "print(f\"{tokenizer.bos_token=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7295651-bf00-4663-8bbc-00dd818e319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 61346, 2231, 11, 279, 10334, 315, 1375, 44515, 5415, 430]\n",
      "[128001, 128001, 128000, 791, 25885, 315, 3728, 24808, 19813, 311, 279]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "<|begin_of_text|>Simply put, the theory of relativity states that\n",
      "<|end_of_text|><|end_of_text|><|begin_of_text|>The phenomenon of global warming refers to the\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "# inputs = tokenizer.tokenize(prompts, return_tensors=\"np\", truncation=True, padding=True, max_length=64)\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "# print(inputs)\n",
    "token_ids = [i.tolist() for i in inputs[\"input_ids\"]]\n",
    "attention_mask = [i.tolist() for i in inputs[\"attention_mask\"]]\n",
    "print(\"\\n\".join([str(ids) for ids in token_ids]))\n",
    "print(\"\\n\".join([str(ids) for ids in attention_mask]))\n",
    "print()\n",
    "outputs = tokenizer.batch_decode(token_ids, skip_special_tokens=False)\n",
    "print(\"\\n\".join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "babc10f0-7648-4a51-9a16-1718614a1ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa8629092554349b92ca6561aa7ef27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e38557-94f9-4e37-8691-25bdb6ae7127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2dc81d3-9b49-4234-8905-66bef6525b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.device=device(type='mps', index=0)\n",
      "model.dtype=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model.device=}\")\n",
    "print(f\"{model.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed2adc1c-ff44-46aa-8bfb-78040e6612ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained.example\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config, unused_kwargs = GenerationConfig.from_pretrained(model_id, return_unused_kwargs=True)\n",
    "print(generation_config)\n",
    "print(unused_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98efdc97-356d-4f50-a3ff-a5cd2f16a85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simply put, the theory of relativity states that the speed of light (speed of electromagnetism) is the same for all observers no matter how fast they may be moving. As one observer travels at a speed of 100 miles per hour, another observer can only see the first observer as moving at 100\n",
      "\n",
      "The phenomenon of global warming refers to the rising of temperature levels in the atmosphere because of the rise in greenhouse gas levels. Greenhouse gases include Carbon dioxide, water vapour, nitrous oxide, methane, and chlorofluorocarbons (CFCs).\n",
      "Greenhouse gases are the main reason\n"
     ]
    }
   ],
   "source": [
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "# print(inputs)\n",
    "\n",
    "# need this again to suppress warning\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate\n",
    "# https://github.com/huggingface/transformers/blob/a22a4378d97d06b7a1d9abad6e0086d30fdea199/src/transformers/generation/utils.py#L1914\n",
    "# `**kwargs` passed to generate matching the attributes of `generation_config` will override them.\n",
    "output_ids = model.generate(**inputs, generation_config=generation_config, max_length=64, temperature=0.9)\n",
    "print()\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "print(\"\\n\\n\".join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45c670f6-fbbd-4e22-b053-ea9cbca104c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context lengths: \n",
    "# https://github.com/huggingface/transformers/blob/2932f318a20d9e54cc7aea052e040164d85de7d6/src/transformers/models/llama/convert_llama_weights_to_hf.py#L96"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
